{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6707124,"sourceType":"datasetVersion","datasetId":3865528},{"sourceId":6834059,"sourceType":"datasetVersion","datasetId":3929074},{"sourceId":7041024,"sourceType":"datasetVersion","datasetId":4050956}],"dockerImageVersionId":30558,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Lectura de data","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-11-24T13:33:27.204842Z","iopub.execute_input":"2023-11-24T13:33:27.205362Z","iopub.status.idle":"2023-11-24T13:33:27.634252Z","shell.execute_reply.started":"2023-11-24T13:33:27.205315Z","shell.execute_reply":"2023-11-24T13:33:27.633087Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = '/kaggle/input/finals/data categorizada.xlsx'\ndf = pd.read_excel(data)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:27.636695Z","iopub.execute_input":"2023-11-24T13:33:27.637906Z","iopub.status.idle":"2023-11-24T13:33:31.886044Z","shell.execute_reply.started":"2023-11-24T13:33:27.637872Z","shell.execute_reply":"2023-11-24T13:33:31.884780Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Eliminar filas con NaN\ndf = df.dropna()\nfilas = df.shape[0]\nconteo = df.groupby('APT Verdict').size()\nconteo","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:31.887480Z","iopub.execute_input":"2023-11-24T13:33:31.887917Z","iopub.status.idle":"2023-11-24T13:33:31.928687Z","shell.execute_reply.started":"2023-11-24T13:33:31.887889Z","shell.execute_reply":"2023-11-24T13:33:31.927497Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Limpieza de texto","metadata":{}},{"cell_type":"code","source":"from bs4 import BeautifulSoup\nimport pandas as pd\nimport re\n\n\ndef limpiar(texto):\n    texto_sincaracter = re.sub(r'[^a-zA-Z\\s]', '',str(texto))\n    texto_min = texto_sincaracter.lower()\n    soup = BeautifulSoup(texto_min, 'html.parser')\n    texto_sinhtml = soup.get_text()\n    texto_limpio = ' '.join(texto_sinhtml.split())\n    return texto_limpio\n\ndf['Subject_limpio'] = df['Subject'].apply(limpiar)\n\nprint(df['Subject_limpio'])","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:31.930906Z","iopub.execute_input":"2023-11-24T13:33:31.931345Z","iopub.status.idle":"2023-11-24T13:33:33.316203Z","shell.execute_reply.started":"2023-11-24T13:33:31.931306Z","shell.execute_reply":"2023-11-24T13:33:33.315047Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop('Subject', axis=1)\nordenar = ['Sender IP', 'From (SMTP)', 'From (Header)', 'Subject_limpio', 'Has Attachment', 'Size', 'APT Verdict']\ndf = df[ordenar]\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:33.319951Z","iopub.execute_input":"2023-11-24T13:33:33.320394Z","iopub.status.idle":"2023-11-24T13:33:33.341310Z","shell.execute_reply.started":"2023-11-24T13:33:33.320361Z","shell.execute_reply":"2023-11-24T13:33:33.339944Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Tokenización","metadata":{}},{"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nimport spacy\n\n# Aplica la tokenización a la columna 'Subject_limpio'\ndf['Tokens'] = df['Subject_limpio'].apply(word_tokenize)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:33.342936Z","iopub.execute_input":"2023-11-24T13:33:33.343253Z","iopub.status.idle":"2023-11-24T13:33:52.083885Z","shell.execute_reply.started":"2023-11-24T13:33:33.343226Z","shell.execute_reply":"2023-11-24T13:33:52.082650Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop('Subject_limpio', axis=1)\nordenar = ['Sender IP', 'From (SMTP)', 'From (Header)', 'Tokens', 'Has Attachment', 'Size', 'APT Verdict']\ndf = df[ordenar]\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:52.085237Z","iopub.execute_input":"2023-11-24T13:33:52.085947Z","iopub.status.idle":"2023-11-24T13:33:52.106148Z","shell.execute_reply.started":"2023-11-24T13:33:52.085913Z","shell.execute_reply":"2023-11-24T13:33:52.104929Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Eliminar stop words","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport nltk\nfrom nltk.corpus import stopwords\n\nnltk.download('stopwords')\nstop_words = set(stopwords.words('spanish'))\n\ndef eliminar_stopwords(tokens):\n    return [token for token in tokens if token.lower() not in stop_words]\n\ndf['Tokens_limpios'] = df['Tokens'].apply(eliminar_stopwords)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:52.107325Z","iopub.execute_input":"2023-11-24T13:33:52.107645Z","iopub.status.idle":"2023-11-24T13:33:52.241666Z","shell.execute_reply.started":"2023-11-24T13:33:52.107605Z","shell.execute_reply":"2023-11-24T13:33:52.240456Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop('Tokens', axis=1)\nordenar = ['Sender IP', 'From (SMTP)', 'From (Header)', 'Tokens_limpios', 'Has Attachment', 'Size', 'APT Verdict']\ndf = df[ordenar]\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:52.243416Z","iopub.execute_input":"2023-11-24T13:33:52.243919Z","iopub.status.idle":"2023-11-24T13:33:52.265799Z","shell.execute_reply.started":"2023-11-24T13:33:52.243872Z","shell.execute_reply":"2023-11-24T13:33:52.264663Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Lematización","metadata":{}},{"cell_type":"code","source":"!pip install spacy\n!python -m spacy download es_core_news_sm","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:33:52.267333Z","iopub.execute_input":"2023-11-24T13:33:52.267685Z","iopub.status.idle":"2023-11-24T13:34:31.209250Z","shell.execute_reply.started":"2023-11-24T13:33:52.267654Z","shell.execute_reply":"2023-11-24T13:34:31.207894Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import spacy\n\n# Carga el modelo de spaCy en español\nnlp = spacy.load(\"es_core_news_sm\")\n\ndef lematizar(tokens):\n    lematizar = []\n    for token in tokens:\n        doc = nlp(token)\n        lematizar.append(doc[0].lemma_)\n    return lematizar\n\ndf['Tokens_lematizados'] = df['Tokens_limpios'].apply(lematizar)\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:34:31.211798Z","iopub.execute_input":"2023-11-24T13:34:31.213201Z","iopub.status.idle":"2023-11-24T13:42:44.151759Z","shell.execute_reply.started":"2023-11-24T13:34:31.213151Z","shell.execute_reply":"2023-11-24T13:42:44.150585Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = df.drop('Tokens_limpios', axis=1)\nordenar = ['Sender IP', 'From (SMTP)', 'From (Header)', 'Tokens_lematizados', 'Has Attachment', 'Size', 'APT Verdict']\ndf = df[ordenar]\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:42:44.153537Z","iopub.execute_input":"2023-11-24T13:42:44.153907Z","iopub.status.idle":"2023-11-24T13:42:44.175824Z","shell.execute_reply.started":"2023-11-24T13:42:44.153877Z","shell.execute_reply":"2023-11-24T13:42:44.174575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Eliminar las filas de Tokens_lematizados que tienen vectores vacios (=[])\ndf = df[df['Tokens_lematizados'].apply(lambda x: len(x) > 0)]\ndf.shape[0]","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:42:44.177389Z","iopub.execute_input":"2023-11-24T13:42:44.178656Z","iopub.status.idle":"2023-11-24T13:42:44.198395Z","shell.execute_reply.started":"2023-11-24T13:42:44.178599Z","shell.execute_reply":"2023-11-24T13:42:44.197315Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Word2Vec","metadata":{}},{"cell_type":"markdown","source":"## SKIP-GRAM","metadata":{}},{"cell_type":"code","source":"df_w2v_skip = df.copy()\nprint(df_w2v_skip.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:42:44.203447Z","iopub.execute_input":"2023-11-24T13:42:44.204039Z","iopub.status.idle":"2023-11-24T13:42:44.218364Z","shell.execute_reply.started":"2023-11-24T13:42:44.204005Z","shell.execute_reply":"2023-11-24T13:42:44.217107Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom gensim.models import Word2Vec\n\n# Construir el modelo Word2Vec y construir el vocabulario\nmodel = Word2Vec(sentences=df_w2v_skip['Tokens_lematizados'], vector_size=50, sg=1, window=5, min_count=1, workers=4)\n\n# Entrenar el modelo Word2Vec\nmodel.train(df_w2v_skip['Tokens_lematizados'], total_examples=len(df_w2v_skip['Tokens_lematizados']), epochs=10)\n\n# Función para obtener el vector de una lista de tokens\ndef get_vector(tokens):\n    return model.wv[set(tokens) & set(model.wv.index_to_key)]\n\n# Función para obtener el vector promedio de una lista de tokens\ndef get_average_vector(tokens):\n    vector_sum = np.zeros(50)  # Inicializar un vector de ceros con la misma dimensión que los vectores de palabras\n    num_vectors = 0\n    for token in tokens:\n        if token in model.wv:\n            vector_sum += model.wv[token]\n            num_vectors += 1\n    if num_vectors > 0:\n        return vector_sum / num_vectors\n    else:\n        return vector_sum\n\n# Aplicar la función a cada fila del DataFrame\ndf_w2v_skip['w2v_skip'] = df_w2v_skip['Tokens_lematizados'].apply(get_average_vector)\n\n# Imprimir el DataFrame resultante\nprint(df_w2v_skip)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:42:44.219923Z","iopub.execute_input":"2023-11-24T13:42:44.221014Z","iopub.status.idle":"2023-11-24T13:43:24.701783Z","shell.execute_reply.started":"2023-11-24T13:42:44.220972Z","shell.execute_reply":"2023-11-24T13:43:24.700697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_w2v_skip = df_w2v_skip.drop('Tokens_lematizados', axis=1)\nordenar = ['Sender IP', 'From (SMTP)', 'From (Header)', 'w2v_skip', 'Has Attachment', 'Size', 'APT Verdict']\ndf_w2v_skip = df_w2v_skip[ordenar]\nprint(df_w2v_skip.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:24.703212Z","iopub.execute_input":"2023-11-24T13:43:24.703662Z","iopub.status.idle":"2023-11-24T13:43:24.725072Z","shell.execute_reply.started":"2023-11-24T13:43:24.703628Z","shell.execute_reply":"2023-11-24T13:43:24.723903Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n# Definir una función para convertir un vector NumPy en una lista\ndef numpy_vector_to_list(vector):\n    return vector.tolist()\n\n# Aplicar la función a la columna y crear una nueva columna de listas\ndf_w2v_skip['w2v_skip'] = df_w2v_skip['w2v_skip'].apply(numpy_vector_to_list)\n\n# Mostrar el DataFrame resultante\nprint(df_w2v_skip)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:24.726356Z","iopub.execute_input":"2023-11-24T13:43:24.726719Z","iopub.status.idle":"2023-11-24T13:43:25.191768Z","shell.execute_reply.started":"2023-11-24T13:43:24.726689Z","shell.execute_reply":"2023-11-24T13:43:25.190524Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Función para agregar números de IP a vectores\ndef agregar_ip_a_vector(ip1, ip2, ip3, vector):\n    numeros_ip1 = [float(numero) for numero in ip1.split('.')]\n    numeros_ip2 = [float(numero) for numero in ip2.split('.')]\n    numeros_ip3 = [float(numero) for numero in ip3.split('.')]\n    \n    nuevo_vector = vector + numeros_ip1 + numeros_ip2 + numeros_ip3\n    return nuevo_vector\n\n# Aplicar la función a cada fila del DataFrame\ndf_w2v_skip['Concatenado_ips'] = df_w2v_skip.apply(lambda row: agregar_ip_a_vector(row['Sender IP'], row['From (SMTP)'], row['From (Header)'], row['w2v_skip']), axis=1)\n\n# Función para agregar enteros y floats como elementos individuales en los vectores\ndef agregar_numero_a_vector(vector, numero):\n    nuevo_vector = vector + [float(numero)]\n    return nuevo_vector\n\n# Aplicar la función a cada fila del DataFrame para enteros\ndf_w2v_skip['Concatenado_total'] = df_w2v_skip.apply(lambda row: agregar_numero_a_vector(row['Concatenado_ips'], row['Has Attachment']), axis=1)\n\n# Aplicar la función a cada fila del DataFrame para floats\ndf_w2v_skip['Concatenado_total'] = df_w2v_skip.apply(lambda row: agregar_numero_a_vector(row['Concatenado_total'], row['Size']), axis=1)\n\n# Mostrar el DataFrame resultante\nprint(df_w2v_skip.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:25.192983Z","iopub.execute_input":"2023-11-24T13:43:25.193307Z","iopub.status.idle":"2023-11-24T13:43:26.700899Z","shell.execute_reply.started":"2023-11-24T13:43:25.193281Z","shell.execute_reply":"2023-11-24T13:43:26.699499Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"first_row = df_w2v_skip[\"Concatenado_total\"].loc[1]\nprint(first_row)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:26.702541Z","iopub.execute_input":"2023-11-24T13:43:26.703199Z","iopub.status.idle":"2023-11-24T13:43:26.712470Z","shell.execute_reply.started":"2023-11-24T13:43:26.703168Z","shell.execute_reply":"2023-11-24T13:43:26.708673Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reemplazar \"no phishing\" por 0 y \"phishing\" por 1\ndf_w2v_skip['APT Verdict'] = df_w2v_skip['APT Verdict'].replace({'no phishing': 0, 'phishing': 1})\nprint(df_w2v_skip.head())","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:26.714290Z","iopub.execute_input":"2023-11-24T13:43:26.714670Z","iopub.status.idle":"2023-11-24T13:43:26.765511Z","shell.execute_reply.started":"2023-11-24T13:43:26.714639Z","shell.execute_reply":"2023-11-24T13:43:26.763927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### RANDOM FOREST","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\n# Separar las características (vectores) y las etiquetas\nX = df_w2v_skip['Concatenado_total'].tolist()\ny = df_w2v_skip['APT Verdict']\n\n# Dividir los datos en conjuntos de entrenamiento y prueba\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)\n\n# Definir los hiperparámetros a sintonizar\nparam_grid = {\n    'n_estimators': [100, 200, 300],  # Número de árboles en el bosque\n    'max_depth': [None, 5, 10],  # Profundidad máxima de cada árbol\n    'min_samples_split': [2, 5, 10],  # Número mínimo de muestras requeridas para dividir un nodo\n    'min_samples_leaf': [1, 2, 4]  # Número mínimo de muestras requeridas en un nodo hoja\n}\n\n# Crear un modelo de Random Forest\nrf_classifier = RandomForestClassifier(random_state=2023)\n\n# Realizar la búsqueda de cuadrícula\ngrid_search = GridSearchCV(rf_classifier, param_grid, cv=5)\ngrid_search.fit(X_train, y_train)\n\n# Obtener el mejor modelo con los hiperparámetros sintonizados\nbest_model = grid_search.best_estimator_\n\n# Realizar predicciones en el conjunto de prueba con el mejor modelo\ny_pred = best_model.predict(X_test)\n\n# Calcular el accuracy\naccuracy = accuracy_score(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n\n# Calcular la matriz de confusión\nconfusion = confusion_matrix(y_test, y_pred)\n\n# Calcular la precisión\nprecision = precision_score(y_test, y_pred, average='weighted')\nprint(f'Precision: {precision}')\n\n# Calcular el recall\nrecall = recall_score(y_test, y_pred, average='weighted')\nprint(f'Recall: {recall}')\n\n# Calcular el F-score\nf1 = f1_score(y_test, y_pred, average='weighted')\nprint(f'F-Score: {f1}')\n\n# Calcular la especificidad\ntn, fp, fn, tp = confusion.ravel()\nspecificity = tn / (tn + fp)\nprint(f'Specificity: {specificity}')","metadata":{"execution":{"iopub.status.busy":"2023-11-24T13:43:26.767511Z","iopub.execute_input":"2023-11-24T13:43:26.768202Z","iopub.status.idle":"2023-11-24T14:38:37.054699Z","shell.execute_reply.started":"2023-11-24T13:43:26.768143Z","shell.execute_reply":"2023-11-24T14:38:37.053534Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n# Configurar el gráfico\nplt.figure(figsize=(6, 4))\nsns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False,\n            xticklabels=['Clase Negativa', 'Clase Positiva'],\n            yticklabels=['Clase Negativa', 'Clase Positiva'])\n\nplt.xlabel('Predicción')\nplt.ylabel('Valor Real')\nplt.title('Matriz de Confusión')\n\n# Mostrar el gráfico\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-11-24T14:38:37.056230Z","iopub.execute_input":"2023-11-24T14:38:37.056577Z","iopub.status.idle":"2023-11-24T14:38:37.648460Z","shell.execute_reply.started":"2023-11-24T14:38:37.056547Z","shell.execute_reply":"2023-11-24T14:38:37.646943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# FIN DEL CÓDIGO","metadata":{}},{"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"execution":{"iopub.status.busy":"2023-11-24T14:38:37.651418Z","iopub.execute_input":"2023-11-24T14:38:37.652658Z","iopub.status.idle":"2023-11-24T14:38:37.676076Z","shell.execute_reply.started":"2023-11-24T14:38:37.652582Z","shell.execute_reply":"2023-11-24T14:38:37.674927Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = '/kaggle/input/data-simulando/data original simulacion.xlsx'\ndf = pd.read_excel(data)\ndf","metadata":{"execution":{"iopub.status.busy":"2023-11-24T14:47:22.245119Z","iopub.execute_input":"2023-11-24T14:47:22.245524Z","iopub.status.idle":"2023-11-24T14:47:23.572532Z","shell.execute_reply.started":"2023-11-24T14:47:22.245494Z","shell.execute_reply":"2023-11-24T14:47:23.571222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom bs4 import BeautifulSoup\nimport pandas as pd\nimport re\nfrom nltk.tokenize import word_tokenize\nimport spacy\nimport os\nimport nltk\nfrom nltk.corpus import stopwords\nimport pandas as pd\nfrom gensim.models import Word2Vec\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nfrom sklearn.metrics import confusion_matrix\n\ndef predecir(df):\n    \n    #Eliminar filas con NaN\n    df = df.dropna()\n    conteo = df.groupby('APT Verdict').size()\n    print(conteo)\n    \n    # Realizar Limpieza de texto\n    def limpiar(texto):\n        texto_sincaracter = re.sub(r'[^a-zA-Z\\s]', '',str(texto))\n        texto_min = texto_sincaracter.lower()\n        soup = BeautifulSoup(texto_min, 'html.parser')\n        texto_sinhtml = soup.get_text()\n        texto_limpio = ' '.join(texto_sinhtml.split())\n        return texto_limpio\n    \n    df['Subject'] = df['Subject'].apply(limpiar)\n\n    # Realizar Tokenización\n    df['Subject'] = df['Subject'].apply(word_tokenize)\n    \n    # Realizar Eliminación de stop words\n    nltk.download('stopwords')\n    stop_words = set(stopwords.words('spanish'))\n    def eliminar_stopwords(tokens):\n        return [token for token in tokens if token.lower() not in stop_words]\n    \n    df['Subject'] = df['Subject'].apply(eliminar_stopwords)\n    \n    # Realizar Lematización\n    #!pip install spacy\n    #!python -m spacy download es_core_news_sm\n    nlp = spacy.load(\"es_core_news_sm\")\n\n    def lematizar(tokens):\n        lematizar = []\n        for token in tokens:\n            doc = nlp(token)\n            lematizar.append(doc[0].lemma_)\n        return lematizar\n    \n    df['Subject'] = df['Subject'].apply(lematizar)\n    \n    #Eliminar las filas de Tokens_lematizados que tienen vectores vacios (=[])\n    df = df[df['Subject'].apply(lambda x: len(x) > 0)]\n    conteo2 = df.groupby('APT Verdict').size()\n    print(conteo2)\n    \n    # Modelo Word2Vec\n    model = Word2Vec(sentences=df['Subject'], vector_size=50, sg=1, window=5, min_count=1, workers=4)\n    model.train(df['Subject'], total_examples=len(df['Subject']), epochs=10)\n    \n    def get_average_vector(tokens):\n        vector_sum = np.zeros(50)\n        num_vectors = 0\n        for token in tokens:\n            if token in model.wv:\n                vector_sum += model.wv[token]\n                num_vectors += 1\n        if num_vectors > 0:\n            return vector_sum / num_vectors\n        else:\n            return vector_sum\n        \n    df['Subject'] = df['Subject'].apply(get_average_vector)\n\n    def numpy_vector_to_list(vector):\n        return vector.tolist()\n    \n    df['Subject'] = df['Subject'].apply(numpy_vector_to_list)\n    \n    def agregar_ip_a_vector(ip1, ip2, ip3, vector):\n        numeros_ip1 = [float(numero) for numero in ip1.split('.')]\n        numeros_ip2 = [float(numero) for numero in ip2.split('.')]\n        numeros_ip3 = [float(numero) for numero in ip3.split('.')]\n\n        nuevo_vector = vector + numeros_ip1 + numeros_ip2 + numeros_ip3\n        return nuevo_vector\n\n    df['Concatenado_total'] = df.apply(lambda row: agregar_ip_a_vector(row['Sender IP'], row['From (SMTP)'], row['From (Header)'], row['Subject']), axis=1)\n\n    def agregar_numero_a_vector(vector, numero):\n        nuevo_vector = vector + [float(numero)]\n        return nuevo_vector\n    \n    df['Concatenado_total'] = df.apply(lambda row: agregar_numero_a_vector(row['Concatenado_total'], row['Has Attachment']), axis=1)\n    df['Concatenado_total'] = df.apply(lambda row: agregar_numero_a_vector(row['Concatenado_total'], row['Size']), axis=1)\n    \n    df['APT Verdict'] = df['APT Verdict'].replace({'no phishing': 0, 'phishing': 1})\n    \n    # Predecir con modelo Random Forest\n    X_input = df['Concatenado_total'].tolist()\n    y_real = df['APT Verdict']\n    \n    y_pred = best_model.predict(X_input)\n\n    # Imprime la predicción\n    print(\"Predicción:\", y_pred)\n    \n    # Calcular el accuracy\n    accuracy = accuracy_score(y_real, y_pred)\n    print(f'Accuracy: {accuracy}')\n\n    # Calcular la matriz de confusión\n    confusion = confusion_matrix(y_real, y_pred)\n\n    # Calcular la precisión\n    precision = precision_score(y_real, y_pred, average='weighted')\n    print(f'Precision: {precision}')\n\n    # Calcular el recall\n    recall = recall_score(y_real, y_pred, average='weighted')\n    print(f'Recall: {recall}')\n\n    # Calcular el F-score\n    f1 = f1_score(y_real, y_pred, average='weighted')\n    print(f'F-Score: {f1}')\n\n    # Calcular la especificidad\n    tn, fp, fn, tp = confusion.ravel()\n    specificity = tn / (tn + fp)\n    print(f'Specificity: {specificity}')","metadata":{"execution":{"iopub.status.busy":"2023-11-24T14:57:11.130584Z","iopub.execute_input":"2023-11-24T14:57:11.131043Z","iopub.status.idle":"2023-11-24T14:57:11.163095Z","shell.execute_reply.started":"2023-11-24T14:57:11.130999Z","shell.execute_reply":"2023-11-24T14:57:11.161567Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predecir(df)","metadata":{"execution":{"iopub.status.busy":"2023-11-24T15:05:22.921250Z","iopub.execute_input":"2023-11-24T15:05:22.921709Z","iopub.status.idle":"2023-11-24T15:08:55.671369Z","shell.execute_reply.started":"2023-11-24T15:05:22.921673Z","shell.execute_reply":"2023-11-24T15:08:55.670069Z"},"trusted":true},"outputs":[],"execution_count":null}]}